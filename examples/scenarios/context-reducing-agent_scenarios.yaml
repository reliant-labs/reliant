# Scenario tests for context-reducing-agent workflow
#
# This workflow has a loop (agent_loop) with nodes:
#   call_llm -> (manual mode) approval -> execute_tools
#            -> (auto mode) execute_tools
#   execute_tools -> (large results > 4000 chars) filter_results -> execute_filter -> save_filtered_results -> compact
#                 -> (small results) save_tool_results -> compact
#
# The loop continues while tool_calls is not empty and iteration < max_turns

---
# Happy path: Agent responds with text only in auto mode
name: text_only_auto_mode
description: Agent responds with text, no tool calls - loop exits
inputs:
  mode: "auto"
events:
  - node: agent_loop.call_llm
    output:
      message:
        role: assistant
        text: "Here's my answer to your question."
      response_text: "Here's my answer to your question."
      tool_calls: []
expect:
  outcome: completed
  reached:
    - agent_loop
    - agent_loop.call_llm
  not_reached:
    - agent_loop.approval
    - agent_loop.execute_tools

---
# Auto mode with small tool results
name: auto_mode_small_results
description: Auto mode with tool calls producing small results - direct save path
inputs:
  mode: "auto"
events:
  # First iteration: LLM requests a tool
  - node: agent_loop.call_llm
    output:
      message:
        role: assistant
        text: "Let me check that file."
      response_text: "Let me check that file."
      tool_calls:
        - id: call_0
          name: view
          input:
            file_path: "/path/to/small-file.go"
  # Tool returns small result (< 4000 chars)
  - node: agent_loop.execute_tools
    output:
      message:
        role: tool
        text: ""
      tool_results:
        - tool_call_id: call_0
          content: 'package main\n\nfunc hello() string {\n\treturn "world"\n}'
      total_result_chars: 50
      thread_token_count: 500
  # Second iteration: final response
  - node: agent_loop.call_llm
    output:
      message:
        role: assistant
        text: "The file contains a simple hello function."
      response_text: "The file contains a simple hello function."
      tool_calls: []
expect:
  outcome: completed
  reached:
    - agent_loop
    - agent_loop.call_llm
    - agent_loop.execute_tools
    - agent_loop.save_tool_results
  not_reached:
    - agent_loop.approval
    - agent_loop.filter_results
    - agent_loop.execute_filter
    - agent_loop.save_filtered_results

---
# Auto mode with large tool results - triggers filtering
name: auto_mode_large_results_filtered
description: Auto mode with large results triggers filter path
inputs:
  mode: "auto"
events:
  # First iteration: LLM requests a tool
  - node: agent_loop.call_llm
    output:
      message:
        role: assistant
        text: "Let me search the codebase."
      response_text: "Let me search the codebase."
      tool_calls:
        - id: call_0
          name: grep
          input:
            pattern: "import"
            path: "."
  # Tool returns large result (> 4000 chars)
  - node: agent_loop.execute_tools
    output:
      message:
        role: tool
        text: ""
      tool_results:
        - tool_call_id: call_0
          content: "lots of results here..."
      total_result_chars: 5000
      thread_token_count: 500
  # filter_results LLM call
  - node: agent_loop.filter_results
    output:
      message:
        role: assistant
        text: ""
      response_text: ""
      tool_calls:
        - id: call_1
          name: filtered_results
          input:
            results:
              - tool_call_id: "call_0"
                name: "grep"
                filtered: true
                content: "Summarized: 50 import statements found"
                is_error: false
  # execute_filter result
  - node: agent_loop.execute_filter
    output:
      message:
        role: tool
        text: ""
      tool_results:
        - tool_call_id: call_1
          content: '{"results": [{"tool_call_id": "call_0", "filtered": true, "content": "Summarized: 50 import statements found"}]}'
      response_data:
        filtered_results:
          results:
            - tool_call_id: "call_0"
              name: "grep"
              filtered: true
              content: "Summarized: 50 import statements found"
              is_error: false
      thread_token_count: 600
  # Second iteration: final response
  - node: agent_loop.call_llm
    output:
      message:
        role: assistant
        text: "Found 50 import statements across the codebase."
      response_text: "Found 50 import statements across the codebase."
      tool_calls: []
expect:
  outcome: completed
  reached:
    - agent_loop
    - agent_loop.call_llm
    - agent_loop.execute_tools
    - agent_loop.filter_results
    - agent_loop.execute_filter
    - agent_loop.save_filtered_results
  not_reached:
    - agent_loop.approval
    - agent_loop.save_tool_results

---
# Manual mode - requires approval
name: manual_mode_approved
description: Manual mode requires approval before executing tools
inputs:
  mode: "manual"
events:
  # LLM requests a tool
  - node: agent_loop.call_llm
    output:
      message:
        role: assistant
        text: "I need to edit this file."
      response_text: "I need to edit this file."
      tool_calls:
        - id: call_0
          name: edit
          input:
            file_path: "/path/to/file.go"
            old_string: "old"
            new_string: "new"
  # User approves
  - node: agent_loop.approval
    output:
      approval_id: "approval_1"
      status: "approved"
      action_taken: "approve"
      data: {}
  # Tool executes with small result
  - node: agent_loop.execute_tools
    output:
      message:
        role: tool
        text: ""
      tool_results:
        - tool_call_id: call_0
          content: "File edited successfully"
      total_result_chars: 100
      thread_token_count: 500
  # Final response
  - node: agent_loop.call_llm
    output:
      message:
        role: assistant
        text: "File has been updated."
      response_text: "File has been updated."
      tool_calls: []
expect:
  outcome: completed
  reached:
    - agent_loop
    - agent_loop.call_llm
    - agent_loop.approval
    - agent_loop.execute_tools
    - agent_loop.save_tool_results

---
# Manual mode - denied approval ends the flow
name: manual_mode_denied
description: Manual mode with denied approval - tool execution skipped
inputs:
  mode: "manual"
events:
  # LLM requests a tool
  - node: agent_loop.call_llm
    output:
      message:
        role: assistant
        text: "I want to run a dangerous command."
      response_text: "I want to run a dangerous command."
      tool_calls:
        - id: call_0
          name: bash
          input:
            command: "rm -rf /"
  # User denies
  - node: agent_loop.approval
    output:
      approval_id: "approval_1"
      status: "denied"
      action_taken: "deny"
      data: {}
expect:
  outcome: completed
  reached:
    - agent_loop
    - agent_loop.call_llm
    - agent_loop.approval
  not_reached:
    - agent_loop.execute_tools
    - agent_loop.save_tool_results

---
# Compaction triggered after small results
name: compaction_after_small_results
description: High token count triggers compaction after saving small results
inputs:
  mode: "auto"
  compaction_threshold: 10000
events:
  # LLM requests a tool
  - node: agent_loop.call_llm
    output:
      message:
        role: assistant
        text: "Let me check that."
      response_text: "Let me check that."
      tool_calls:
        - id: call_0
          name: view
          input:
            file_path: "test.go"
  # Tool result with high thread token count
  - node: agent_loop.execute_tools
    output:
      message:
        role: tool
        text: ""
      tool_results:
        - tool_call_id: call_0
          content: "package test"
      total_result_chars: 100
      thread_token_count: 15000
  # After save, compact should trigger
  # Final response
  - node: agent_loop.call_llm
    output:
      message:
        role: assistant
        text: "Done."
      response_text: "Done."
      tool_calls: []
expect:
  outcome: completed
  reached:
    - agent_loop
    - agent_loop.call_llm
    - agent_loop.execute_tools
    - agent_loop.save_tool_results
    - agent_loop.compact

---
# Compaction triggered after filtered results
name: compaction_after_filtered_results
description: High token count triggers compaction after saving filtered results
inputs:
  mode: "auto"
  compaction_threshold: 10000
events:
  # LLM requests a tool
  - node: agent_loop.call_llm
    output:
      message:
        role: assistant
        text: "Searching..."
      response_text: "Searching..."
      tool_calls:
        - id: call_0
          name: grep
          input:
            pattern: "error"
            path: "."
  # Large result triggers filtering
  - node: agent_loop.execute_tools
    output:
      message:
        role: tool
        text: ""
      tool_results:
        - tool_call_id: call_0
          content: "many errors..."
      total_result_chars: 6000
      thread_token_count: 15000
  # filter_results LLM
  - node: agent_loop.filter_results
    output:
      message:
        role: assistant
        text: ""
      response_text: ""
      tool_calls:
        - id: call_1
          name: filtered_results
          input:
            results:
              - tool_call_id: "call_0"
                name: "grep"
                filtered: true
                content: "5 critical errors found"
                is_error: false
  # execute_filter result
  - node: agent_loop.execute_filter
    output:
      message:
        role: tool
        text: ""
      tool_results:
        - tool_call_id: call_1
          content: '{"results": [{"tool_call_id": "call_0", "filtered": true, "content": "5 critical errors found"}]}'
      response_data:
        filtered_results:
          results:
            - tool_call_id: "call_0"
              name: "grep"
              filtered: true
              content: "5 critical errors found"
              is_error: false
      thread_token_count: 15000
  # Final response
  - node: agent_loop.call_llm
    output:
      message:
        role: assistant
        text: "Found 5 critical errors."
      response_text: "Found 5 critical errors."
      tool_calls: []
expect:
  outcome: completed
  reached:
    - agent_loop
    - agent_loop.call_llm
    - agent_loop.execute_tools
    - agent_loop.filter_results
    - agent_loop.execute_filter
    - agent_loop.save_filtered_results
    - agent_loop.compact

---
# Compaction skipped after small results when token count is below threshold
name: compaction_skipped_after_small_results
description: Token count stays below compaction_threshold after saving small results, compact is not reached
inputs:
  mode: "auto"
  compaction_threshold: 185000
events:
  # LLM requests a tool
  - node: agent_loop.call_llm
    output:
      message:
        role: assistant
        text: "Let me check that."
      response_text: "Let me check that."
      tool_calls:
        - id: call_0
          name: view
          input:
            file_path: "test.go"
  # Tool result with low token count (below threshold)
  - node: agent_loop.execute_tools
    output:
      message:
        role: tool
        text: ""
      tool_results:
        - tool_call_id: call_0
          content: "package test"
      total_result_chars: 100
      thread_token_count: 500
  # Compact is skipped, next iteration starts
  - node: agent_loop.call_llm
    output:
      message:
        role: assistant
        text: "Done."
      response_text: "Done."
      tool_calls: []
expect:
  outcome: completed
  reached:
    - agent_loop
    - agent_loop.call_llm
    - agent_loop.execute_tools
    - agent_loop.save_tool_results
  not_reached:
    - agent_loop.compact
    - agent_loop.filter_results

---
# Compaction skipped after filtered results when token count is below threshold
name: compaction_skipped_after_filtered_results
description: Token count stays below compaction_threshold after saving filtered results, compact is not reached
inputs:
  mode: "auto"
  compaction_threshold: 185000
events:
  # LLM requests a tool
  - node: agent_loop.call_llm
    output:
      message:
        role: assistant
        text: "Let me analyze that file."
      response_text: "Let me analyze that file."
      tool_calls:
        - id: call_0
          name: view
          input:
            file_path: "large.go"
  # Large tool result triggers filtering
  - node: agent_loop.execute_tools
    output:
      message:
        role: tool
        text: ""
      tool_results:
        - tool_call_id: call_0
          content: "Very long content that exceeds 4000 chars..."
      total_result_chars: 5000
      thread_token_count: 1000
  # Filter LLM evaluates the result
  - node: agent_loop.filter_results
    output:
      message:
        role: assistant
        text: ""
      response_text: ""
      tool_calls:
        - id: call_1
          name: filtered_results
          input:
            results:
              - tool_call_id: "call_0"
                name: "view"
                filtered: true
                content: "Contains Go code with multiple functions."
                is_error: false
  # Execute filter tool
  - node: agent_loop.execute_filter
    output:
      message:
        role: tool
        text: ""
      tool_results:
        - tool_call_id: call_1
          content: '{"results": [{"tool_call_id": "call_0", "filtered": true, "content": "Contains Go code with multiple functions."}]}'
      response_data:
        filtered_results:
          results:
            - tool_call_id: "call_0"
              name: "view"
              filtered: true
              content: "Contains Go code with multiple functions."
              is_error: false
      thread_token_count: 1200
  # Compact is skipped (token count 1200 < threshold 185000), next iteration starts
  - node: agent_loop.call_llm
    output:
      message:
        role: assistant
        text: "Analysis complete."
      response_text: "Analysis complete."
      tool_calls: []
expect:
  outcome: completed
  reached:
    - agent_loop
    - agent_loop.call_llm
    - agent_loop.execute_tools
    - agent_loop.filter_results
    - agent_loop.execute_filter
    - agent_loop.save_filtered_results
  not_reached:
    - agent_loop.compact
    - agent_loop.save_tool_results

---
# Plan mode - read-only tools only
name: plan_mode_readonly
description: Plan mode restricts to plan-tagged tools
inputs:
  mode: "plan"
events:
  # LLM uses read-only tool
  - node: agent_loop.call_llm
    output:
      message:
        role: assistant
        text: "Let me look at the code structure."
      response_text: "Let me look at the code structure."
      tool_calls:
        - id: call_0
          name: glob
          input:
            pattern: "**/*.go"
  # Tool returns small result
  - node: agent_loop.execute_tools
    output:
      message:
        role: tool
        text: ""
      tool_results:
        - tool_call_id: call_0
          content: '["main.go", "util.go"]'
      total_result_chars: 30
      thread_token_count: 500
  # Final response
  - node: agent_loop.call_llm
    output:
      message:
        role: assistant
        text: "Found 2 Go files."
      response_text: "Found 2 Go files."
      tool_calls: []
expect:
  outcome: completed
  reached:
    - agent_loop
    - agent_loop.call_llm
    - agent_loop.execute_tools
    - agent_loop.save_tool_results
  not_reached:
    - agent_loop.approval

---
# Max turns limit scenario - loop exits when iteration limit reached
name: max_turns_limit_reached
description: Loop exits when iter.iteration reaches inputs.max_turns
inputs:
  mode: "auto"
  max_turns: 2
events:
  # Iteration 1: LLM requests a tool
  - node: agent_loop.call_llm
    output:
      message:
        role: assistant
        text: "Let me search for files."
      response_text: "Let me search for files."
      tool_calls:
        - id: call_0
          name: grep
          input:
            pattern: "TODO"
            path: "."
  - node: agent_loop.execute_tools
    output:
      message:
        role: tool
        text: ""
      tool_results:
        - tool_call_id: call_0
          content: '["main.go:10", "util.go:25"]'
      total_result_chars: 30
      thread_token_count: 500
  # Iteration 2 (max_turns=2, so this is the last allowed iteration)
  - node: agent_loop.call_llm
    output:
      message:
        role: assistant
        text: "Found some TODOs. Let me check the first file."
      response_text: "Found some TODOs. Let me check the first file."
      tool_calls:
        - id: call_1
          name: view
          input:
            file_path: "main.go"
  - node: agent_loop.execute_tools
    output:
      message:
        role: tool
        text: ""
      tool_results:
        - tool_call_id: call_1
          content: "// TODO: implement this"
      total_result_chars: 25
      thread_token_count: 600
  # Loop exits after iteration 2 because iter.iteration (2) is no longer < max_turns (2)
expect:
  outcome: completed
  reached:
    - agent_loop
    - agent_loop.call_llm
    - agent_loop.execute_tools
    - agent_loop.save_tool_results
  not_reached:
    - agent_loop.approval
    - agent_loop.filter_results
